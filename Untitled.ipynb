{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b0065ed6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/aryankarki/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/aryankarki/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/aryankarki/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  \\\n",
      "0  Today's News   \\n(Friday, December 6th, 2024) ...   \n",
      "1  Good Friday - Bhagavad Gita Chapter 9, Verse 2...   \n",
      "2  The Nepal Stock Exchange (NEPSE) index decreas...   \n",
      "3  HRL aligns well with its fundamentals, showing...   \n",
      "4  eyeframe tales \\nfor educational purpose only ...   \n",
      "\n",
      "                                        cleaned_text  \n",
      "0                      today news friday december th  \n",
      "1  good friday bhagavad gita chapter verse assura...  \n",
      "2  nepal stock exchange nepse index decreased poi...  \n",
      "3  hrl aligns well fundamental showing promising ...  \n",
      "4                  eyeframe tale educational purpose  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "\n",
    "# Download NLTK resources (run this once)\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Load the dataset\n",
    "nepse_tweets = pd.read_csv(\"nepse_tweets.csv\")  # Replace with your dataset's path\n",
    "\n",
    "# Initialize the lemmatizer and stopwords\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Function for preprocessing\n",
    "def preprocess_text(text):\n",
    "    # Remove URLs\n",
    "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", '', text, flags=re.MULTILINE)\n",
    "    # Remove mentions and hashtags\n",
    "    text = re.sub(r\"@\\w+|#\\w+\", '', text)\n",
    "    # Remove numbers and special characters\n",
    "    text = re.sub(r\"[^a-zA-Z\\s]\", '', text)\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Tokenize the text\n",
    "    tokens = word_tokenize(text)\n",
    "    # Remove stopwords and lemmatize\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]\n",
    "    # Join tokens back into a string\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Apply preprocessing to the text column\n",
    "nepse_tweets['cleaned_text'] = nepse_tweets['text'].apply(preprocess_text)\n",
    "\n",
    "# Save the cleaned dataset (optional)\n",
    "nepse_tweets.to_csv(\"cleaned_nepse_tweets.csv\", index=False)\n",
    "\n",
    "# Display a preview of the cleaned data\n",
    "print(nepse_tweets[['text', 'cleaned_text']].head())\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "731e70ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Example data creation (replace with your actual data loading process)\n",
    "data = {\n",
    "    \"Today's News\": [\n",
    "        \"Today's News\\n(Friday, December 6th, 2024)...\",\n",
    "        \"Good Friday - Bhagavad Gita Chapter 9, Verse 2...\",\n",
    "        \"The Nepal Stock Exchange (NEPSE) index decreas...\",\n",
    "        \"HRL aligns well with its fundamentals, showing...\",\n",
    "        \"eyeframe tales\\nfor educational purpose only...\",\n",
    "    ],\n",
    "    \"cleaned_text\": [\n",
    "        \"today news friday december th\",\n",
    "        \"good friday bhagavad gita chapter verse assura...\",\n",
    "        \"nepal stock exchange nepse index decreased poi...\",\n",
    "        \"hrl aligns well fundamental showing promising...\",\n",
    "        \"eyeframe tale educational purpose\",\n",
    "    ]\n",
    "}\n",
    "df = pd.DataFrame(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "390d78f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file_path = \"processed_tweets.csv\"\n",
    "df.to_csv(output_file_path, index=False)\n",
    "\n",
    "print(f\"Data saved successfully to {output_file_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7714d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file_path = \"processed_tweets.csv\"\n",
    "nepse_tweets.to_csv(output_file_path, index=False)\n",
    "\n",
    "print(f\"Data saved successfully to {output_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a8634c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aryankarki/anaconda3/lib/python3.11/site-packages/transformers/utils/generic.py:260: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "/Users/aryankarki/anaconda3/lib/python3.11/site-packages/transformers/utils/generic.py:260: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from transformers import pipeline\n",
    "from datetime import datetime\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(\"processed_tweets.csv\")\n",
    "\n",
    "# Ensure 'created_at' is parsed as datetime if it's present in your dataset\n",
    "if 'created_at' in df.columns:\n",
    "    df['created_at'] = pd.to_datetime(df['created_at'])\n",
    "\n",
    "### TEXT FEATURES ###\n",
    "\n",
    "# 1. TF-IDF Vectorization\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=100)  # Adjust max_features as needed\n",
    "tfidf_features = tfidf_vectorizer.fit_transform(df['cleaned_text']).toarray()\n",
    "\n",
    "# Convert to DataFrame for better handling\n",
    "tfidf_df = pd.DataFrame(tfidf_features, columns=tfidf_vectorizer.get_feature_names_out())\n",
    "\n",
    "# 2. Embedding using BERT (Optional for semantic understanding)\n",
    "# Load a BERT model pipeline for feature extraction\n",
    "bert_pipeline = pipeline('feature-extraction', model='bert-base-uncased', tokenizer='bert-base-uncased')\n",
    "\n",
    "# Generate embeddings (optional, comment if not needed for now)\n",
    "# df['bert_embeddings'] = df['cleaned_text'].apply(lambda x: bert_pipeline(x)[0][0])  # [CLS] token embeddings\n",
    "\n",
    "### METADATA FEATURES ###\n",
    "\n",
    "# Normalize engagement metrics (assuming columns like 'favorite_count', 'retweet_count', 'reply_count' exist)\n",
    "engagement_columns = ['favorite_count', 'retweet_count', 'reply_count']  # Adjust to your actual column names\n",
    "if all(col in df.columns for col in engagement_columns):\n",
    "    scaler = MinMaxScaler()\n",
    "    engagement_features = scaler.fit_transform(df[engagement_columns])\n",
    "    engagement_df = pd.DataFrame(engagement_features, columns=[f'normalized_{col}' for col in engagement_columns])\n",
    "else:\n",
    "    engagement_df = pd.DataFrame()\n",
    "\n",
    "# Time-based patterns\n",
    "if 'created_at' in df.columns:\n",
    "    df['hour'] = df['created_at'].dt.hour  # Extract hour\n",
    "    df['day_of_week'] = df['created_at'].dt.dayofweek  # Extract day of the week\n",
    "\n",
    "### FINAL DATAFRAME ###\n",
    "\n",
    "# Combine all features\n",
    "final_df = pd.concat(\n",
    "    [\n",
    "        tfidf_df, \n",
    "        engagement_df, \n",
    "        df[['hour', 'day_of_week']] if 'created_at' in df.columns else pd.DataFrame()\n",
    "    ],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Save the processed features to a new CSV\n",
    "output_file = \"features_processed_tweets.csv\"\n",
    "final_df.to_csv(output_file, index=False)\n",
    "print(f\"Features saved successfully to {output_file}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c54e8dc4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
